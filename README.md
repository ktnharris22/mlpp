# MLP 94-889
Personal Homework Repo for Machine Learning for Public Policy at Heinz College at Carnegie Mellon University

## Assignment 1: ETL of ACS Data


#### Extraction
I chose to use the API that is directly provided by the Census. It seemed like the most readily available and there is a lot of documentation available for how it works.

Using the census API I chose to pull demographic data about race, gender, and household income. I chose these variables because they seemed informative and foundational for most cases.
In the main function, I take input from the user. Currently it only takes state and county as inputs, but given more time I would have liked to include more options so that the data extraction was more dynamic. After taking user input, main calls buildDF which takes the paramaters provided by the user and gets the data requested by the user using getData. getData makes the call to the API and returns the json parsed response. 

#### Transformation
 From there buildDF, builds a pandas dataframe using the parsed repsonse. It renames the columns and drops the first header row so once written to a csv it can be easily uploaded using COPY FROM. I chose to use copy_from from the psycopg2 library rather than iterating over rows or using pandas DataFrame.to_sql function beacause it is much more efficient. This does come at a cost of flexibility though. If I want to change the variables I'm uploading in any shape or form I will have to change the SQL statement in the code to do so. My reasoning is that I'd rather optimize for the loading of data then for the functionality of being able to change which variables are uploaded without changing any code. In my experience you load data more often then you change the data it is your loading, so I chose to optimize for the operation I anticipate doing more often. Changing the variables that are loaded is still doable and simple. The user only would have to change the CREATE TABLE sql statement after adding the variables to the list. Also because I anticipate I'm the only user for this program I'm not too concerned about the burden put on the user by doing it this way. I think this way also empowers the user more because they have more direct control of how the table is getting created. If other people were going to use this I might have chosen flexility over efficiency.

#### Loading
Config.py, loads the configuration for the connection to the database based off of some hidden parameters in a .ini file. The second file acs.py uses Config.py to connect to the database. I could have just directly connected to the database in acs.py but then I would have had to write the login credentials in a publicly available file and that is bad practice. I found Config.py online and created my .ini file so it was an easy choice for extra security. Although this data is all publicly available anyways, the data we will be working on in the future will not be.

After exporting the pandas data frame to a csv, acs.py connects to the database, creates the table, then uploads all the data in the csv. While there are <10k rows for this exercise, I anticipate loading larger amounts of data in the future which is another reason why I chose to use copy_from. createTable always drops and then recreates the table to account for any changes and all changes in the table structure or data. The primary key in this instance I chose was a combination of the state, county, tract, and block_grp. This should be a unique identifier for each and every row. I thought about using an auto-incremented key but in the end decided this would work in this case given there are <10k rows in the table, and it isn't likely anyone would be trying to join to my table. I also could have included the state name, county name, tract name, etc in this table but almost certainly in a real database there would be separate tables for those and state, county, etc. in my table would be foreign keys to the tables that have the names of the geographic locations. After the insert is done (via copy_from) the program prints how many rows were inserted and the connection to the database is closed. 